#!/usr/bin/env python3
"""
Async AI Task Runner ä¸­ç­‰è´Ÿè½½æµ‹è¯•å·¥å…·

ç”¨é€”: æµ‹è¯•ç³»ç»Ÿåœ¨ä¸­ç­‰å¹¶å‘è´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
ç”¨æ³•: python load-test-medium.py [--concurrent 50] [--requests 10] [--url http://localhost:8000]
"""

import asyncio
import aiohttp
import time
import json
import argparse
import statistics
from datetime import datetime
from typing import List, Dict, Any, Tuple

class LoadTestResult:
    def __init__(self):
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.response_times = []
        self.errors = []
        self.start_time = None
        self.end_time = None
        self.created_tasks = []

    def add_success(self, response_time: float, task_data: Dict[str, Any]):
        self.successful_requests += 1
        self.response_times.append(response_time)
        self.created_tasks.append(task_data)

    def add_failure(self, error: str):
        self.failed_requests += 1
        self.errors.append(error)

    def get_summary(self) -> Dict[str, Any]:
        if not self.response_times:
            return {
                'avg_response_time': 0,
                'min_response_time': 0,
                'max_response_time': 0,
                'median_response_time': 0
            }

        return {
            'avg_response_time': statistics.mean(self.response_times),
            'min_response_time': min(self.response_times),
            'max_response_time': max(self.response_times),
            'median_response_time': statistics.median(self.response_times)
        }

async def create_task(session: aiohttp.ClientSession, url: str, task_id: int) -> Tuple[float, Dict[str, Any], str]:
    """åˆ›å»ºå•ä¸ªä»»åŠ¡å¹¶è¿”å›ç»“æœ"""
    task_data = {
        "prompt": f"è´Ÿè½½æµ‹è¯•ä»»åŠ¡ {task_id}ï¼šè¯·è®¡ç®— {task_id} Ã— 2",
        "model": "deepseek-chat",
        "priority": task_id % 5 + 1
    }

    start_time = time.time()
    error_message = ""

    try:
        async with session.post(
            f"{url}/api/v1/tasks",
            json=task_data,
            headers={"accept": "application/json"}
        ) as response:
            response_text = await response.text()
            response_time = time.time() - start_time

            if response.status == 200:
                try:
                    response_data = json.loads(response_text)
                    return response_time, response_data, ""
                except json.JSONDecodeError:
                    error_message = f"Invalid JSON: {response_text[:100]}"
                    return response_time, {}, error_message
            else:
                error_message = f"HTTP {response.status}: {response_text[:100]}"
                return response_time, {}, error_message

    except Exception as e:
        response_time = time.time() - start_time
        error_message = f"Request failed: {str(e)}"
        return response_time, {}, error_message

async def run_load_test(concurrent: int, requests_per_batch: int, base_url: str) -> LoadTestResult:
    """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
    result = LoadTestResult()
    result.start_time = datetime.now()

    print(f"ğŸš€ å¼€å§‹ä¸­ç­‰è´Ÿè½½æµ‹è¯•")
    print(f"å¹¶å‘æ•°: {concurrent}")
    print(f"æ¯æ‰¹è¯·æ±‚æ•°: {requests_per_batch}")
    print(f"æ€»è¯·æ±‚æ•°: {concurrent * requests_per_batch}")
    print("-" * 50)

    connector = aiohttp.TCPConnector(limit=concurrent * 2, limit_per_host=concurrent)
    timeout = aiohttp.ClientTimeout(total=30)

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        for batch in range(requests_per_batch):
            print(f"æ‰§è¡Œç¬¬ {batch + 1}/{requests_per_batch} æ‰¹è¯·æ±‚...")

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = []
            for i in range(concurrent):
                task_id = batch * concurrent + i + 1
                task = create_task(session, base_url, task_id)
                tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for response_time, task_data, error in batch_results:
                result.total_requests += 1

                if isinstance(task_data, dict) and task_data.get('id') and not error:
                    result.add_success(response_time, task_data)
                else:
                    result.add_failure(error if error else "Unknown error")

            # æ˜¾ç¤ºæ‰¹æ¬¡ç»Ÿè®¡
            batch_success = sum(1 for r in batch_results if isinstance(r, tuple) and r[1].get('id'))
            avg_time = statistics.mean([r[0] for r in batch_results if isinstance(r, tuple)]) if batch_results else 0

            print(f"æ‰¹æ¬¡ {batch + 1}: æˆåŠŸ {batch_success}/{concurrent}, å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")

            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
            if batch < requests_per_batch - 1:
                await asyncio.sleep(1)

    result.end_time = datetime.now()
    return result

def print_results(result: LoadTestResult):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    print("-" * 50)
    print("ğŸ“Š è´Ÿè½½æµ‹è¯•ç»“æœ")
    print("-" * 50)

    duration = (result.end_time - result.start_time).total_seconds()

    print(f"æµ‹è¯•æ—¶é—´: {result.start_time.strftime('%Y-%m-%d %H:%M:%S')} - {result.end_time.strftime('%Y-%m:%d:%S')}")
    print(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
    print(f"æ€»è¯·æ±‚æ•°: {result.total_requests}")
    print(f"æˆåŠŸè¯·æ±‚: {result.successful_requests}")
    print(f"å¤±è´¥è¯·æ±‚: {result.failed_requests}")
    print(f"æˆåŠŸç‡: {(result.successful_requests / result.total_requests * 100):.1f}%")

    if result.response_times:
        stats = result.get_summary()
        print(f"\nğŸ“ˆ å“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}s")
        print(f"  æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.3f}s")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.3f}s")
        print(f"  ä¸­ä½æ•°å“åº”æ—¶é—´: {stats['median_response_time']:.3f}s")

        # å“åº”æ—¶é—´åˆ†å¸ƒ
        response_buckets = {
            '< 100ms': sum(1 for t in result.response_times if t < 0.1),
            '100-200ms': sum(1 for t in result.response_times if 0.1 <= t < 0.2),
            '200-500ms': sum(1 for t in result.response_times if 0.2 <= t < 0.5),
            '500ms-1s': sum(1 for t in result.response_times if 0.5 <= t < 1.0),
            '>= 1s': sum(1 for t in result.response_times if t >= 1.0)
        }

        print(f"\nğŸ“Š å“åº”æ—¶é—´åˆ†å¸ƒ:")
        for bucket, count in response_buckets.items():
            percentage = count / len(result.response_times) * 100
            print(f"  {bucket}: {count} ({percentage:.1f}%)")

    # æ€§èƒ½æŒ‡æ ‡
    rps = result.total_requests / duration if duration > 0 else 0
    success_rps = result.successful_requests / duration if duration > 0 else 0

    print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ€»ååé‡: {rps:.2f} RPS (æ¯ç§’è¯·æ±‚æ•°)")
    print(f"  æˆåŠŸååé‡: {success_rps:.2f} RPS")
    print(f"  å¹³å‡ QPS: {result.successful_requests / (result.end_time - result.start_time).total_seconds():.2f}")

    # é”™è¯¯ç»Ÿè®¡
    if result.errors:
        print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
        error_counts = {}
        for error in result.errors:
            error_type = error.split(':')[0] if ':' in error else 'Other'
            error_counts[error_type] = error_counts.get(error_type, 0) + 1

        for error_type, count in error_counts.items():
            print(f"  {error_type}: {count}")

    # æˆåŠŸä»»åŠ¡IDèŒƒå›´
    if result.created_tasks:
        task_ids = [task['id'] for task in result.created_tasks if task.get('id')]
        print(f"\nâœ… æˆåŠŸåˆ›å»ºçš„ä»»åŠ¡IDèŒƒå›´: {min(task_ids)} - {max(task_ids)}")

def save_results(result: LoadTestResult, filename: str = None):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"load_test_result_{timestamp}.json"

    stats = result.get_summary()

    report_data = {
        "test_info": {
            "timestamp": result.start_time.isoformat(),
            "duration_seconds": (result.end_time - result.start_time).total_seconds(),
            "total_requests": result.total_requests,
            "successful_requests": result.successful_requests,
            "failed_requests": result.failed_requests,
            "success_rate": result.successful_requests / result.total_requests * 100
        },
        "performance_stats": {
            "avg_response_time": stats['avg_response_time'],
            "min_response_time": stats['min_response_time'],
            "max_response_time': stats['max_response_time'],
            "median_response_time": stats['median_response_time']
        },
        "created_tasks": result.created_tasks,
        "errors": result.errors
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")

async def main():
    parser = argparse.ArgumentParser(description='Async AI Task Runner è´Ÿè½½æµ‹è¯•å·¥å…·')
    parser.add_argument('--concurrent', type=int, default=50, help='å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 50)')
    parser.add_argument('--requests', type=int, default=10, help='æ¯æ‰¹è¯·æ±‚æ•° (é»˜è®¤: 10)')
    parser.add_argument('--url', type=str, default='http://localhost:8000', help='API åŸºç¡€ URL (é»˜è®¤: http://localhost:8000)')
    parser.add_argument('--save', action='store_true', help='ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶')

    args = parser.parse_args()

    print("ğŸ”¬ Async AI Task Runner ä¸­ç­‰è´Ÿè½½æµ‹è¯•")
    print("=" * 50)

    try:
        # è¿è¡Œè´Ÿè½½æµ‹è¯•
        result = await run_load_test(args.concurrent, args.requests, args.url)

        # æ‰“å°ç»“æœ
        print_results(result)

        # ä¿å­˜ç»“æœ
        if args.save:
            save_results(result)

        # è®¾ç½®é€€å‡ºç 
        if result.failed_requests == 0:
            print("\nğŸ‰ æ‰€æœ‰è¯·æ±‚éƒ½æˆåŠŸå®Œæˆï¼")
            exit(0)
        elif result.successful_requests / result.total_requests > 0.95:
            print(f"\nâš ï¸ å¤§éƒ¨åˆ†è¯·æ±‚æˆåŠŸ ({result.successful_requests}/{result.total_requests})")
            exit(0)
        else:
            print(f"\nâŒ å¤±è´¥è¯·æ±‚è¿‡å¤š ({result.failed_requests}/{result.total_requests})")
            exit(1)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())