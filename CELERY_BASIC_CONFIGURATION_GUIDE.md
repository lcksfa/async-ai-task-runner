# é…ç½®åŸºæœ¬ Celery å®ä¾‹è¯¦ç»†æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è§£é‡Šå¦‚ä½•åœ¨ Async AI Task Runner é¡¹ç›®ä¸­é…ç½®åŸºæœ¬çš„ Celery å®ä¾‹ï¼Œä»é›¶å¼€å§‹åˆ°ç”Ÿäº§å°±ç»ªçš„å®Œæ•´é…ç½®è¿‡ç¨‹ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£ Celery åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„
- æŒæ¡ Celery å®ä¾‹çš„åˆ›å»ºå’Œé…ç½®
- å­¦ä¼šä»»åŠ¡å®šä¹‰å’Œè°ƒç”¨æ–¹æ³•
- æŒæ¡ Worker å¯åŠ¨å’Œç›‘æ§
- äº†è§£é«˜çº§é…ç½®é€‰é¡¹

---

## ğŸ—ï¸ Celery åŸºç¡€æ¦‚å¿µ

### æ ¸å¿ƒç»„ä»¶æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    æ¶ˆæ¯     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ä»»åŠ¡    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer    â”‚  â—€â”€â”€â”€â”€â”€â”€â–¶   â”‚    Broker   â”‚  â—€â”€â”€â”€â”€â”€â”€â–¶   â”‚   Consumer  â”‚
â”‚  (ç”Ÿäº§è€…)    â”‚           â”‚  (æ¶ˆæ¯é˜Ÿåˆ—)  â”‚           â”‚  (æ¶ˆè´¹è€…)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                           â†“                           â†“
  åº”ç”¨ç¨‹åº                      Redis                       Worker
  FastAPI                      ä»»åŠ¡é˜Ÿåˆ—                      Celery
```

### å…³é”®æœ¯è¯­è§£é‡Š

| æœ¯è¯­ | è¯´æ˜ | åœ¨æˆ‘ä»¬é¡¹ç›®ä¸­çš„å®ç° |
|------|------|-------------------|
| **Task** | è¦æ‰§è¡Œçš„å·¥ä½œå•å…ƒ | `run_ai_text_generation()` |
| **Worker** | æ‰§è¡Œä»»åŠ¡çš„è¿›ç¨‹ | `celery -A app.worker worker` |
| **Broker** | æ¶ˆæ¯ä¸­é—´ä»¶ï¼Œå­˜å‚¨å¾…å¤„ç†ä»»åŠ¡ | `redis://localhost:6379/1` |
| **Backend** | å­˜å‚¨ä»»åŠ¡ç»“æœçš„åœ°æ–¹ | `redis://localhost:6379/2` |
| **Queue** | ä»»åŠ¡é˜Ÿåˆ—ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç±» | `ai_processing`, `demo_tasks` |

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. ä¾èµ–å®‰è£…

**æ–‡ä»¶**: [`pyproject.toml`](pyproject.toml:19-21)

```toml
dependencies = [
    # ... å…¶ä»–ä¾èµ–
    "celery[redis]>=5.3.0",    # Celery æ ¸å¿ƒå’Œ Redis æ”¯æŒ
    "redis>=5.0.0",            # Redis Python å®¢æˆ·ç«¯
    "flower>=2.0.0",           # Celery ç›‘æ§å·¥å…·
]
```

**å®‰è£…å‘½ä»¤**:
```bash
# ä½¿ç”¨ uvï¼ˆæˆ‘ä»¬çš„é¡¹ç›®æ¨èï¼‰
uv sync

# æˆ–è€…ä½¿ç”¨ pip
pip install "celery[redis]>=5.3.0" "redis>=5.0.0" "flower>=2.0.0"
```

### 2. Redis æœåŠ¡å¯åŠ¨

```bash
# ä½¿ç”¨ Docker å¯åŠ¨ Redisï¼ˆæ¨èï¼‰
docker run -d --name redis-ai-task -p 6379:6379 redis:7-alpine

# éªŒè¯ Redis è¿æ¥
docker exec redis-ai-task redis-cli ping
# åº”è¯¥è¿”å›: PONG

# æˆ–è€…ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ Redis
redis-server --daemonize yes
redis-cli ping
```

### 3. PostgreSQL æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰

```bash
# å¯åŠ¨ PostgreSQL
docker run -d --name async-ai-postgres -p 5433:5432 \
  -e POSTGRES_DB=task_runner \
  -e POSTGRES_USER=taskuser \
  -e POSTGRES_PASSWORD=taskpass \
  postgres:16
```

---

## ğŸ¯ ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºåŸºæœ¬ Celery åº”ç”¨

### 1.1 æœ€å°åŒ–é…ç½®ç¤ºä¾‹

```python
# minimal_celery_app.py
from celery import Celery

# æœ€ç®€å•çš„ Celery åº”ç”¨
app = Celery(
    'myapp',                    # åº”ç”¨åç§°
    broker='redis://localhost:6379/0',  # Redis æ¶ˆæ¯ä»£ç†
    backend='redis://localhost:6379/0'   # ç»“æœå­˜å‚¨
)

@app.task
def add(x, y):
    """æœ€ç®€å•çš„ä»»åŠ¡å®šä¹‰"""
    return x + y
```

### 1.2 æˆ‘ä»¬é¡¹ç›®çš„é…ç½®å®ç°

**æ–‡ä»¶**: [`app/worker/app.py`](app/worker/app.py)

```python
from celery import Celery
from app.core.config import settings

# ğŸ¯ åˆ›å»º Celery åº”ç”¨å®ä¾‹
celery_app = Celery(
    "async_ai_task_runner",              # åº”ç”¨åç§°
    broker=settings.celery_broker_url,  # æ¶ˆæ¯ä»£ç†åœ°å€
    backend=settings.celery_result_backend,  # ç»“æœå­˜å‚¨åœ°å€
    include=[                           # åŒ…å«çš„ä»»åŠ¡æ¨¡å—
        "app.worker.tasks.ai_tasks",
        "app.worker.tasks.demo_tasks"
    ]
)

# âš™ï¸ é«˜çº§é…ç½®ï¼ˆç¨åè¯¦ç»†è§£é‡Šï¼‰
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)
```

### 1.3 ç¯å¢ƒå˜é‡é…ç½®

**æ–‡ä»¶**: [`app/core/config.py`](app/core/config.py:13-16)

```python
class Settings(BaseSettings):
    # Redis & Celery é…ç½®
    redis_url: str = "redis://localhost:6379/0"
    celery_broker_url: str = "redis://localhost:6379/1"      # ğŸ“¤ ä»»åŠ¡é˜Ÿåˆ—
    celery_result_backend: str = "redis://localhost:6379/2"  # ğŸ’¾ ç»“æœå­˜å‚¨

    class Config:
        env_file = ".env"
        case_sensitive = False
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ä¸åŒçš„Redisæ•°æ®åº“ï¼Ÿ**
- `redis://localhost:6379/1` - å­˜å‚¨ä»»åŠ¡é˜Ÿåˆ—ï¼ˆBrokerï¼‰
- `redis://localhost:6379/2` - å­˜å‚¨ä»»åŠ¡ç»“æœï¼ˆBackendï¼‰
- **å¥½å¤„**: æ•°æ®åˆ†ç¦»ï¼Œä¾¿äºç®¡ç†å’Œæ¸…ç†

---

## âš™ï¸ ç¬¬äºŒæ­¥ï¼šé«˜çº§é…ç½®è¯¦è§£

### 2.1 åºåˆ—åŒ–é…ç½®

```python
# åœ¨ app/worker/app.py ä¸­
celery_app.conf.update(
    # ğŸ”„ åºåˆ—åŒ–é…ç½®
    task_serializer="json",           # ä»»åŠ¡æ•°æ®å¦‚ä½•åºåˆ—åŒ–
    accept_content=["json"],          # Workeræ¥å—çš„å†…å®¹ç±»å‹
    result_serializer="json",         # ç»“æœæ•°æ®å¦‚ä½•åºåˆ—åŒ–

    # â° æ—¶é—´é…ç½®
    timezone="UTC",                   # ä½¿ç”¨UTCæ—¶é—´
    enable_utc=True,                  # å¯ç”¨UTCæ—¶é—´
)
```

**ä¸ºä»€ä¹ˆè¦ç”¨JSONåºåˆ—åŒ–ï¼Ÿ**
- âœ… **å¯è¯»æ€§å¥½**: äººç±»å¯è¯»çš„æ•°æ®æ ¼å¼
- âœ… **è½»é‡çº§**: æ¯”pickleåºåˆ—åŒ–æ›´è½»é‡
- âœ… **è·¨è¯­è¨€**: æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€
- âœ… **å®‰å…¨æ€§**: é¿å…pickleçš„å®‰å…¨é£é™©

### 2.2 ä»»åŠ¡è·¯ç”±é…ç½®

```python
# åœ¨ app/worker/app.py ä¸­
celery_app.conf.update(
    # ğŸ¯ ä»»åŠ¡è·¯ç”±é…ç½® - æŒ‰åŠŸèƒ½åˆ†ç¦»ä»»åŠ¡
    task_routes={
        "app.worker.tasks.ai_tasks.*": {"queue": "ai_processing"},
        "app.worker.tasks.demo_tasks.*": {"queue": "demo_tasks"},
        "app.worker.tasks.urgent.*": {"queue": "urgent"},
    }
)
```

**è·¯ç”±ä¼˜åŠ¿**ï¼š
- âœ… **è´Ÿè½½åˆ†ç¦»**: ä¸åŒç±»å‹ä»»åŠ¡ç”±ä¸åŒçš„Workerå¤„ç†
- âœ… **æ€§èƒ½ä¼˜åŒ–**: å¯ä»¥ä¸ºä¸åŒé˜Ÿåˆ—é…ç½®ä¸åŒçš„Worker
- âœ… **æ•…éšœéš”ç¦»**: ä¸€ä¸ªé˜Ÿåˆ—çš„é—®é¢˜ä¸å½±å“å…¶ä»–é˜Ÿåˆ—

**å¯åŠ¨ç‰¹å®šé˜Ÿåˆ—çš„Worker**:
```bash
# åªå¤„ç†AIä»»åŠ¡çš„Worker
celery -A app.worker worker --queues=ai_processing

# åªå¤„ç†ç´§æ€¥ä»»åŠ¡çš„Worker
celery -A app.worker worker --queues=urgent

# å¤„ç†æ‰€æœ‰ä»»åŠ¡çš„Worker
celery -A app.worker worker --queues=ai_processing,demo_tasks,urgent
```

### 2.3 æ€§èƒ½é…ç½®

```python
# åœ¨ app/worker/app.py ä¸­
celery_app.conf.update(
    # ğŸš€ Workeræ€§èƒ½é…ç½®
    worker_prefetch_multiplier=1,    # Workeré¢„å–ä»»åŠ¡æ•°ï¼ˆé‡è¦ï¼ï¼‰
    task_acks_late=True,           # ä»»åŠ¡å®Œæˆåå†ç¡®è®¤ï¼ˆå¯é æ€§ï¼‰

    # ğŸ“Š ç»“æœå’Œç›‘æ§é…ç½®
    result_expires=3600,            # ç»“æœè¿‡æœŸæ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
    worker_send_task_events=True,   # å¯ç”¨ä»»åŠ¡äº‹ä»¶è¿½è¸ª

    # ğŸ›¡ï¸ å¯é æ€§é…ç½®
    worker_disable_rate_limits=False,  # ç¦ç”¨é€Ÿç‡é™åˆ¶
    task_reject_on_worker_lost=True, # Workerä¸¢å¤±æ—¶æ‹’ç»ä»»åŠ¡
)
```

**é‡è¦é…ç½®è¯´æ˜**ï¼š

#### `worker_prefetch_multiplier=1`
```python
# å€¼ä¸º1ï¼ˆæ¨èï¼‰ï¼š
# Workerä¸€æ¬¡åªé¢„å–1ä¸ªä»»åŠ¡ï¼Œå®Œæˆä»»åŠ¡åå†å–ä¸‹ä¸€ä¸ª
# é˜²æ­¢Workerè¿‡è½½ï¼Œç¡®ä¿ä»»åŠ¡æŒ‰é¡ºåºå¤„ç†

# å€¼>1ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰ï¼š
# Workerå¯ä»¥é¢„å–å¤šä¸ªä»»åŠ¡
# é€‚åˆå¿«é€Ÿå°ä»»åŠ¡ï¼Œä½†å¯èƒ½å¯¼è‡´ä»»åŠ¡å †ç§¯
```

#### `task_acks_late=True`
```python
# Trueï¼ˆæ¨èï¼‰ï¼š
# ä»»åŠ¡æˆåŠŸå®Œæˆåæ‰å‘é€ACKç¡®è®¤
# å¦‚æœWorkerå´©æºƒï¼Œä»»åŠ¡ä¼šè¢«é‡æ–°åˆ†é…ç»™å…¶ä»–Worker
# ç¡®ä¿ä»»åŠ¡ä¸ä¸¢å¤±

# Falseï¼ˆé»˜è®¤ï¼‰ï¼š
# Workeræ¥æ”¶åˆ°ä»»åŠ¡åç«‹å³å‘é€ACKç¡®è®¤
# å¦‚æœWorkerå´©æºƒï¼Œä»»åŠ¡ä¼šä¸¢å¤±
# æ€§èƒ½æ›´å¥½ï¼Œä½†å¯é æ€§è¾ƒä½
```

### 2.4 ç›‘æ§é…ç½®

```python
# åœ¨ app/worker/app.py ä¸­
celery_app.conf.update(
    # ğŸ“Š å¯ç”¨ç›‘æ§åŠŸèƒ½
    worker_send_task_events=True,   # å‘é€ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
    task_send_sent_event=True,      # å‘é€ä»»åŠ¡å‘é€äº‹ä»¶

    # ğŸ“ˆ æ€§èƒ½ç›‘æ§
    task_track_started=True,        # è·Ÿè¸ªä»»åŠ¡å¼€å§‹æ—¶é—´
    task_track_completed=True,      # è·Ÿè¸ªä»»åŠ¡å®Œæˆæ—¶é—´
)
```

---

## ğŸ“ ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰ä»»åŠ¡

### 3.1 åŸºç¡€ä»»åŠ¡å®šä¹‰

**æ–‡ä»¶**: [`app/worker/tasks/demo_tasks.py`](app/worker/tasks/demo_tasks.py:11-25)

```python
from app.worker.app import celery_app
import time

@celery_app.task(name="simple_calculation")  # æ˜ç¡®ä»»åŠ¡åç§°ï¼ˆæ¨èï¼‰
def simple_calculation(a: int, b: int, operation: str = "add"):
    """
    ç®€å•çš„æ•°å­¦è®¡ç®—ä»»åŠ¡
    ç”¨äºæµ‹è¯• Celery åŸºæœ¬åŠŸèƒ½
    """
    print(f"ğŸ”¢ å¼€å§‹è®¡ç®—: {a} {operation} {b}")

    # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
    time.sleep(2)

    if operation == "add":
        result = a + b
    elif operation == "multiply":
        result = a * b
    elif operation == "subtract":
        result = a - b
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œ: {operation}")

    print(f"âœ… è®¡ç®—ç»“æœ: {result}")

    return {
        "operation": f"{a} {operation} {b}",
        "result": result,
        "timestamp": time.time()
    }
```

### 3.2 é«˜çº§ä»»åŠ¡ç‰¹æ€§

#### 3.2.1 å¸¦è¿›åº¦è·Ÿè¸ªçš„ä»»åŠ¡

**æ–‡ä»¶**: [`app/worker/tasks/ai_tasks.py`](app/worker/tasks/ai_tasks.py:12-15)

```python
from app.worker.app import celery_app
from app.models import TaskStatus
from app.crud.task import update_task_status, update_task_result

@celery_app.task(bind=True, name="run_ai_text_generation")  # bind=True
def run_ai_text_generation(self, task_id: str, prompt: str, model: str = "gpt-3.3.5-turbo"):
    """
    ğŸ¯ AIæ–‡æœ¬ç”Ÿæˆä»»åŠ¡
    ç‰¹æ€§ï¼š
    1. bind=True - ç»‘å®šä»»åŠ¡å®ä¾‹ï¼Œæ”¯æŒè¿›åº¦è·Ÿè¸ª
    2. æ•°æ®åº“çŠ¶æ€åŒæ­¥ - å®æ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
    3. é”™è¯¯é‡è¯•æœºåˆ¶ - è‡ªåŠ¨å¤±è´¥æ¢å¤
    """
```

**`bind=True` çš„ä½œç”¨**ï¼š
```python
# self åŒ…å«ä»»åŠ¡å®ä¾‹ä¿¡æ¯
print(f"Task ID: {self.request.id}")           # ä»»åŠ¡å”¯ä¸€ID
print(f"Task name: {self.name}")           # ä»»åŠ¡åç§°
print(f"Retry count: {self.request.retries}") # é‡è¯•æ¬¡æ•°

# ğŸ”‘ å®æ—¶è¿›åº¦æ›´æ–°
self.update_state(
    state='PROGRESS',
    meta={
        'progress': 50,
        'status': 'å¤„ç†ä¸­... 50%'
    }
)
```

#### 3.2.2 å¸¦é‡è¯•æœºåˆ¶çš„ä»»åŠ¡

```python
@celery_app.task(bind=True, max_retries=3)  # æœ€å¤§é‡è¯•3æ¬¡
def robust_task(self, data):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„å¥å£®ä»»åŠ¡
    """
    try:
        # å¯èƒ½å¤±è´¥çš„æ“ä½œ
        result = risky_operation(data)
        return result
    except ConnectionError as exc:
        # ç½‘ç»œé”™è¯¯ - é‡è¯•
        print(f"ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œ60ç§’åé‡è¯•")
        raise self.retry(exc=exc, countdown=60)  # 60ç§’åé‡è¯•
    except ValueError as exc:
        # æ•°æ®é”™è¯¯ - ä¸é‡è¯•ï¼Œç›´æ¥å¤±è´¥
        print(f"âŒ æ•°æ®é”™è¯¯: {exc}")
        raise exc
    except Exception as exc:
        # å…¶ä»–é”™è¯¯ - é‡è¯•
        print(f"ğŸ”„ æœªçŸ¥é”™è¯¯ï¼Œ120ç§’åé‡è¯•")
        raise self.retry(exc=exc, countdown=120)  # 120ç§’åé‡è¯•
```

#### 3.2.3 å®šæ—¶ä»»åŠ¡ï¼ˆå¯é€‰ï¼‰

```python
from celery.schedules import crontab

# é…ç½®å®šæ—¶ä»»åŠ¡
celery_app.conf.beat_schedule = {
    'daily-cleanup': {
        'task': 'app.worker.tasks.maintenance.cleanup_old_tasks',
        'schedule': crontab(hour=2, minute=0),  # æ¯å¤©å‡Œæ™¨2ç‚¹
    },
    'hourly-report': {
        'task': 'app.worker.tasks.reports.generate_hourly_stats',
        'schedule': crontab(minute=0),         # æ¯å°æ—¶
    },
}

# å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
# celery -A app.worker beat --loglevel=info
```

---

## ğŸš€ ç¬¬å››æ­¥ï¼šè¿è¡Œå’Œç›‘æ§

### 4.1 å¯åŠ¨ Worker

#### åŸºç¡€å¯åŠ¨
```bash
# æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼
celery -A app.worker worker --loglevel=info
```

#### ç”Ÿäº§ç¯å¢ƒå¯åŠ¨
```bash
# æ¨èçš„ç”Ÿäº§ç¯å¢ƒé…ç½®
celery -A app.worker worker \
    --loglevel=info \
    --concurrency=4 \           # 4ä¸ªå¹¶å‘è¿›ç¨‹
    --prefetch-multiplier=1 \   # é˜²æ­¢è¿‡è½½
    --max-tasks-per-child=1000 # æ¯1000ä¸ªä»»åŠ¡åé‡å¯
```

#### é˜Ÿåˆ—ä¸“ç”¨å¯åŠ¨
```bash
# åªå¤„ç†AIä»»åŠ¡çš„Worker
celery -A app.worker worker --queues=ai_processing --concurrency=2

# å¯åŠ¨å¤šä¸ªWorkerè¿›ç¨‹
celery -A app.worker worker --concurrency=2 &
celery -A app.worker worker --concurrency=2 &
```

### 4.2 å¯åŠ¨ç›‘æ§å·¥å…·

#### Flower ç›‘æ§é¢æ¿
```bash
# å¯åŠ¨Flower
celery -A app.worker flower --port=5555

# è®¿é—®ç›‘æ§ç•Œé¢
# http://localhost:5555
```

**FloweråŠŸèƒ½**ï¼š
- ğŸ“Š **å®æ—¶ç»Ÿè®¡**: ä»»åŠ¡æ‰§è¡Œæ•°é‡ã€æˆåŠŸç‡
- ğŸ‘¥ **WorkerçŠ¶æ€**: åœ¨çº¿Workeråˆ—è¡¨å’ŒçŠ¶æ€
- ğŸ“ˆ **ä»»åŠ¡å†å²**: ä»»åŠ¡æ‰§è¡Œè®°å½•å’Œæ—¶é—´åˆ†æ
- ğŸ”§ **ä»»åŠ¡ç®¡ç†**: æ‰‹åŠ¨é‡è¯•ã€æ’¤é”€ä»»åŠ¡

#### å‘½ä»¤è¡Œç›‘æ§
```bash
# æŸ¥çœ‹æ´»è·ƒä»»åŠ¡
celery -A app.worker inspect active

# æŸ¥çœ‹æ³¨å†Œçš„ä»»åŠ¡
celery -A app.worker inspect registered

# æŸ¥çœ‹Workerç»Ÿè®¡
celery -A app.worker inspect stats

# æŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦
docker exec redis-ai-task redis-cli llen celery
```

### 4.3 éªŒè¯é…ç½®

#### æ£€æŸ¥é…ç½®
```bash
# æŸ¥çœ‹åº”ç”¨é…ç½®
celery -A app.worker inspect conf

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   'task_serializer': 'json',
#   'accept_content': ['json'],
#   'result_serializer': 'json',
#   'timezone': 'UTC',
#   'task_routes': {...}
# }
```

#### æµ‹è¯•ä»»åŠ¡
```bash
# æµ‹è¯•åŸºæœ¬ä»»åŠ¡
python -c "
from app.worker.tasks.demo_tasks import simple_calculation

# å‘é€ä»»åŠ¡
result = simple_calculation.delay(10, 20, 'add')
print(f'ä»»åŠ¡ID: {result.id}')

# ç­‰å¾…ç»“æœ
print(f'ç»“æœ: {result.get(timeout=10)}')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ä»»åŠ¡ID: abc-123-def-456-ghi
ç»“æœ: {'operation': '10 add 20', 'result': 30, 'timestamp': 1634567890.123}
```

---

## ğŸ” ç¬¬äº”æ­¥ï¼šé›†æˆFastAPI

### 5.1 FastAPI é›†æˆ

**æ–‡ä»¶**: [`app/api/v1/endpoints/tasks.py`](app/api/v1/endpoints/tasks.py:12-49)

```python
from fastapi import APIRouter, Depends, HTTPException, status
from app.worker.tasks.ai_tasks import run_ai_text_generation
from app.crud import task as task_crud

@router.post("/tasks", response_model=TaskResponse, status_code=status.HTTP_201_CREATED)
async def create_task(
    task_in: TaskCreate,
    db: AsyncSession = Depends(get_db)
):
    """
    å¼‚æ­¥ä»»åŠ¡åˆ›å»ºæµç¨‹ï¼š
    1. åˆ›å»ºæ•°æ®åº“è®°å½•ï¼ˆç«‹å³ï¼‰
    2. è§¦å‘Celeryä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰
    3. ç«‹å³è¿”å›å“åº”ï¼ˆå¿«é€Ÿï¼‰
    """
    try:
        # 1. åˆ›å»ºä»»åŠ¡è®°å½•åœ¨æ•°æ®åº“
        task = await task_crud.create_task(db=db, obj_in=task_in)

        # 2. è§¦å‘Celeryä»»åŠ¡è¿›è¡Œå¼‚æ­¥å¤„ç†
        try:
            # ğŸ”‘ ç±»å‹è½¬æ¢ï¼šint â†’ str
            run_ai_text_generation.delay(
                task_id=str(task.id),
                prompt=task.prompt,
                model=task.model or "gpt-3.5-turbo"
            )
            print(f"ğŸš€ Celery task triggered for task_id: {task.id}")
        except Exception as celery_error:
            # ğŸ›¡ï¸ å®¹é”™å¤„ç† - Celeryå¤±è´¥ä¸å½±å“APIå“åº”
            print(f"âš ï¸ Failed to trigger Celery task: {celery_error}")

        # 3. ç«‹å³è¿”å›ä»»åŠ¡ID
        return task

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create task: {str(e)}"
        )
```

### 5.2 å·¥ä½œæµç¨‹åˆ†æ

```
ç”¨æˆ·è¯·æ±‚ â†’ FastAPI
    â†“
1. æ¥æ”¶HTTPè¯·æ±‚
2. éªŒè¯è¯·æ±‚æ•°æ®ï¼ˆPydanticï¼‰
3. åˆ›å»ºæ•°æ®åº“è®°å½•
    â†“
ç«‹å³å“åº” â† ç”¨æˆ·å¾—åˆ°Task ID
    â†“
è§¦å‘Celeryä»»åŠ¡ â†’ Redisé˜Ÿåˆ—
    â†“
Celery Worker â† ä»Redisè·å–ä»»åŠ¡
    â†“
å¼€å§‹å¤„ç†ä»»åŠ¡ â†’ æ›´æ–°çŠ¶æ€ä¸ºPROCESSING
    â†“
å®Œæˆå¤„ç† â†’ æ›´æ–°çŠ¶æ€ä¸ºCOMPLETED
    â†“
æ•°æ®åº“ â† ç»“æœå­˜å‚¨
```

### 5.3 API å“åº”æ—¶é—´å¯¹æ¯”

| åœºæ™¯ | åŒæ­¥å¤„ç† | å¼‚æ­¥å¤„ç† | æ€§èƒ½æå‡ |
|------|----------|----------|----------|
| **ç”¨æˆ·æäº¤ä»»åŠ¡** | ç­‰å¾…10-30ç§’ | <100æ¯«ç§’ | **300å€** |
| **å¹¶å‘ç”¨æˆ·æ•°** | å—è¿æ¥æ•°é™åˆ¶ | ç†è®ºæ— é™åˆ¶ | **çº¿æ€§æ‰©å±•** |
| **ç³»ç»Ÿç¨³å®šæ€§** | å®¹æ˜“è¶…æ—¶å´©æºƒ | é«˜ç¨³å®šæ€§ | **è´¨çš„é£è·ƒ** |

---

## ğŸ› ï¸ ç¬¬å…­æ­¥ï¼šé”™è¯¯å¤„ç†å’Œå¯é æ€§

### 6.1 ä»»åŠ¡é”™è¯¯å¤„ç†

```python
@celery_app.task(bind=True, max_retries=3)
def robust_task(self, data):
    try:
        # ä¸šåŠ¡é€»è¾‘
        result = process_data(data)
        return result
    except ConnectionError as exc:
        # ç½‘ç»œé”™è¯¯ - å¯é‡è¯•
        raise self.retry(exc=exc, countdown=60)
    except ValueError as exc:
        # æ•°æ®é”™è¯¯ - ä¸é‡è¯•
        raise exc
    except Exception as exc:
        # æœªçŸ¥é”™è¯¯ - é‡è¯•
        raise self.retry(exc=exc, countdown=60)
```

### 6.2 æ•°æ®åº“äº‹åŠ¡å®‰å…¨

**æ–‡ä»¶**: [`app/database.py`](app/database.py:46-57)

```python
@contextlib.contextmanager
def get_sync_db_session():
    """åŒæ­¥æ•°æ®åº“ä¼šè¯ - ç¡®ä¿è¿æ¥æ­£ç¡®å…³é—­"""
    session = SyncSessionLocal()
    try:
        yield session
    except Exception:
        session.rollback()  # ğŸ”’ å¼‚å¸¸æ—¶å›æ»š
        raise
    finally:
        session.close()   # ğŸ”’ ç¡®ä¿è¿æ¥å…³é—­
```

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
# âœ… æ­£ç¡®çš„æ•°æ®åº“ä½¿ç”¨æ–¹å¼
@celery_app.task
def db_task():
    with get_sync_db_session() as db:
        result = db.query(Task).all()
    return result

# âŒ é”™è¯¯çš„ä½¿ç”¨æ–¹å¼ï¼ˆä¸èƒ½åœ¨Celeryä¸­ä½¿ç”¨asyncï¼‰
@celery.app.task
async def bad_db_task():
    async with get_db() as db:  # âŒ ä¸èƒ½åœ¨Celeryä¸­ä½¿ç”¨async
        return await db.query(Task).all()
```

### 6.3 ä»»åŠ¡çŠ¶æ€åŒæ­¥

**æ–‡ä»¶**: [`app/crud/task.py`](app/crud/task.py:116-131)

```python
def update_task_status(task_id, status: TaskStatus) -> bool:
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    from app.database import get_sync_db_session

    # ğŸ”‘ å¤„ç†ä¸åŒç±»å‹çš„task_id
    task_id_str = str(task_id) if isinstance(task_id, int) else task_id

    with get_sync_db_session() as db:
        return update_task_status_sync(db, task_id_str, status)
```

**ç±»å‹è½¬æ¢å¤„ç†**ï¼š
```python
def update_task_status_sync(db: Session, task_id: str, status: TaskStatus):
    try:
        # ğŸ”‘ å­—ç¬¦ä¸²IDè½¬æ•´æ•°IDç”¨äºæ•°æ®åº“æŸ¥è¯¢
        int_id = int(task_id)
        task = db.query(Task).filter(Task.id == int_id).first()
    except ValueError:
        # å®¹é”™ï¼šæ”¯æŒå­—ç¬¦ä¸²ID
        task = db.query(Task).filter(Task.id == task_id).first()

    if task:
        task.status = status
        db.commit()
        return True
    return False
```

---

## ğŸ“Š ç¬¬ä¸ƒæ­¥ï¼šç›‘æ§å’Œæ—¥å¿—

### 7.1 æ—¥å¿—é…ç½®

```python
import logging
from celery.utils.log import get_task_logger

# è·å–ä»»åŠ¡æ—¥å¿—è®°å½•å™¨
logger = get_task_logger(__name__)

@celery_app.task
def logged_task(data):
    logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: {data}")

    try:
        result = process_data(data)
        logger.info(f"ä»»åŠ¡å®Œæˆ: {result}")
        return result
    except Exception as exc:
        logger.error(f"ä»»åŠ¡å¤±è´¥: {exc}")
        raise
```

### 7.2 æ€§èƒ½ç›‘æ§

```python
# åœ¨ä»»åŠ¡ä¸­æ·»åŠ æ€§èƒ½ç»Ÿè®¡
@celery_app.task(bind=True)
def monitored_task(self, data):
    import time
    start_time = time.time()

    # ä»»åŠ¡é€»è¾‘
    result = process_data(data)

    end_time = time.time()
    duration = end_time - start_time

    self.update_state(
        state='SUCCESS',
        meta={
            'duration': duration,
            'items_processed': len(data)
        }
    )

    return result
```

### 7.3 å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | ç›‘æ§æ–¹æ³• | å¥åº·é˜ˆå€¼ |
|------|----------|----------|
| **ä»»åŠ¡å»¶è¿Ÿ** | Floweré¢æ¿ | <5ç§’ |
| **ä»»åŠ¡æˆåŠŸç‡** | è®¡ç®—å…¬å¼ | >95% |
| **Workeræ•°é‡** | `inspect stats` | åŒ¹é…é¢„æœŸ |
| **é˜Ÿåˆ—é•¿åº¦** | Rediså‘½ä»¤ | <100 |
| **å†…å­˜ä½¿ç”¨** | ç³»ç»Ÿç›‘æ§ | <80% |

---

## ğŸ”§ é«˜çº§é…ç½®é€‰é¡¹

### 8.1 è¿æ¥æ± é…ç½®

```python
# åœ¨ app/worker/app.py ä¸­
celery_app.conf.update(
    broker_pool_limit=10,              # è¿æ¥æ± å¤§å°
    broker_connection_timeout=30,      # è¿æ¥è¶…æ—¶æ—¶é—´
    broker_transport_options={
        'visibility_timeout': 3600,     # ä»»åŠ¡å¯è§æ€§è¶…æ—¶
        'retry_policy': {
            'timeout': 5.0
        }
    }
)
```

### 8.2 å®šæ—¶ä»»åŠ¡é…ç½®

```python
# æ¯å¤©2ç‚¹æ¸…ç†è¿‡æœŸä»»åŠ¡
celery_app.conf.beat_schedule = {
    'cleanup-old-tasks': {
        'task': 'app.worker.tasks.cleanup_old_tasks',
        'schedule': crontab(hour=2, minute=0),
    },
}
```

### 8.3 Workerè‡ªåŠ¨é‡å¯

```bash
# æ¯1000ä¸ªä»»åŠ¡åé‡å¯Workerï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
celery -A app.worker worker --max-tasks-per-child=1000

# æ¯10åˆ†é’Ÿé‡å¯Workerï¼ˆå®šæœŸé‡å¯ï¼‰
# ä½¿ç”¨supervisorç­‰è¿›ç¨‹ç®¡ç†å·¥å…·
```

### 8.4 å®‰å…¨é…ç½®

```python
# å¯ç”¨ä»»åŠ¡äº‹ä»¶è¿½è¸ªï¼ˆå®‰å…¨å»ºè®®ï¼‰
celery_app.conf.update(
    worker_send_task_events=True,
    task_send_sent_event=True,
    task_send_success_event=True,
)

# ç¦ç”¨ä¸å®‰å…¨çš„åºåˆ—åŒ–æ ¼å¼
celery_app.conf.update(
    accept_content=["json"],  # åªæ¥å—JSON
    task_serializer="json",       # åªä½¿ç”¨JSONåºåˆ—åŒ–
)
```

---

## ğŸ› å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1ï¼šä»»åŠ¡ä¸æ‰§è¡Œ

**ç—‡çŠ¶**: ä»»åŠ¡æäº¤åçŠ¶æ€ä¸€ç›´æ˜¯ PENDING

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥WorkerçŠ¶æ€
celery -A app.worker inspect active

# 2. æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æ³¨å†Œ
celery -A app.worker inspect registered | grep your_task_name

# 3. æŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦
docker exec redis-ai-task redis-cli llen celery

# 4. æ£€æŸ¥Workeræ—¥å¿—
docker-compose logs worker
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å¯Worker
pkill -f "celery.*worker"
celery -A app.worker worker --loglevel=info
```

### é—®é¢˜2ï¼šæ•°æ®åº“è¿æ¥é”™è¯¯

**ç—‡çŠ¶**: Celeryä»»åŠ¡ä¸­æ•°æ®åº“æ“ä½œå¤±è´¥

**åŸå› **: Celeryä»»åŠ¡ä¸èƒ½ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“ä¼šè¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# âŒ é”™è¯¯æ–¹å¼
@celery.task
async def bad_db_task():
    async with get_db() as db:  # âŒ ä¸èƒ½åœ¨Celeryä¸­ä½¿ç”¨async
        return await db.query(Task).all()

# âœ… æ­£ç¡®æ–¹å¼
@celery.task
def good_db_task():
    with get_sync_db_session() as db:  # âœ… ä½¿ç”¨åŒæ­¥ä¼šè¯
        return db.query(Task).all()
```

### é—®é¢˜3ï¼šå†…å­˜æ³„æ¼

**ç—‡çŠ¶**: Workerå†…å­˜æŒç»­å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™åˆ¶æ¯ä¸ªWorkerå¤„ç†çš„ä»»åŠ¡æ•°
celery -A app.worker worker --max-tasks-per-child=1000

# å®šæœŸé‡å¯Worker
# ä½¿ç”¨supervisorç­‰è¿›ç¨‹ç®¡ç†å·¥å…·
```

### é—®é¢˜4ï¼šä»»åŠ¡é‡å¤æ‰§è¡Œ

**ç—‡çŠ¶**: ç›¸åŒä»»åŠ¡è¢«å¤šæ¬¡æ‰§è¡Œ

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿ä»»åŠ¡å¹‚ç­‰æ€§
```python
@celery.task
def idempotent_task(unique_id):
    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if is_processed(unique_id):
        return get_result(unique_id)

    # å¤„ç†ä»»åŠ¡
    result = process_task(unique_id)

    # æ ‡è®°å·²å¤„ç†
    mark_processed(unique_id, result)

    return result
```

---

## ğŸ“š å®Œæ•´é…ç½®ç¤ºä¾‹

### åŸºç¡€é…ç½®

```python
# app/worker/basic_app.py
from celery import Celery

app = Celery(
    'myapp',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0',
)

app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)
```

### ç”Ÿäº§é…ç½®

```python
# app/worker/app.py (æˆ‘ä»¬çš„å®ç°)
from celery import Celery
from app.core.config import settings

celery_app = Celery(
    "async_ai_task_runner",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=[
        "app.worker.tasks.ai_tasks",
        "app.worker.tasks.demo_tasks"
    ]
)

celery_app.conf.update(
    # åºåˆ—åŒ–é…ç½®
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,

    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "app.worker.tasks.ai_tasks.*": {"queue": "ai_processing"},
        "app.worker.tasks.demo_tasks.*": {"queue": "demo_tasks"},
    },

    # æ€§èƒ½é…ç½®
    worker_prefetch_multiplier=1,
    task_acks_late=True,

    # ç›‘æ§é…ç½®
    result_expires=3600,
    worker_send_task_events=True,
    task_send_sent_event=True,

    # å¯é æ€§é…ç½®
    worker_disable_rate_limits=False,
    task_reject_on_worker_lost=True,
)
```

---

## ğŸ¯ æ€»ç»“

### é…ç½®è¦ç‚¹å›é¡¾

1. **åº”ç”¨åˆ›å»º**ï¼šä½¿ç”¨ `Celery()` åˆ›å»ºåº”ç”¨å®ä¾‹
2. **Brokeré…ç½®**ï¼šé…ç½®Redisä½œä¸ºæ¶ˆæ¯ä»£ç†
3. **Backendé…ç½®**ï¼šé…ç½®ç»“æœå­˜å‚¨ä½ç½®
4. **ä»»åŠ¡å®šä¹‰**ï¼šä½¿ç”¨ `@celery_app.task` è£…é¥°å™¨
5. **Workerå¯åŠ¨**ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°å¯åŠ¨å·¥ä½œè¿›ç¨‹
6. **ç›‘æ§é›†æˆ**ï¼šä½¿ç”¨Flowerè¿›è¡Œå®æ—¶ç›‘æ§

### é…ç½®æœ€ä½³å®è·µ

1. **ğŸ”‘ ä½¿ç”¨æ˜ç¡®çš„ä»»åŠ¡åç§°** - `@celery_app.task(name="my_task")`
2. **ğŸ¯ åˆ†ç¦»ä¸åŒç±»å‹ä»»åŠ¡** - ä½¿ç”¨ä»»åŠ¡è·¯ç”±å’Œä¸“ç”¨é˜Ÿåˆ—
3. **ğŸ›¡ï¸ å¯ç”¨ä»»åŠ¡ç¡®è®¤** - `task_acks_late=True`
4. **ğŸ“Š å®Œå–„ç›‘æ§é…ç½®** - å¯ç”¨äº‹ä»¶è¿½è¸ª
5. **âš¡ï¸ è®¾ç½®åˆç†çš„é‡è¯•æœºåˆ¶** - é¿å…æ— é™é‡è¯•

### å®Œæ•´æµç¨‹ç¤ºä¾‹

```bash
# 1. å¯åŠ¨Redis
docker run -d --name redis-ai-task -p 6379:6379 redis:7-alpine

# 2. å¯åŠ¨Worker
celery -A app.worker worker --loglevel=info --concurrency=2

# 3. å¯åŠ¨Flowerï¼ˆå¯é€‰ï¼‰
celery -A app.worker flower --port=5555

# 4. æµ‹è¯•é…ç½®
python -c "
from app.worker.tasks.demo_tasks import simple_calculation
result = simple_calculation.delay(10, 20, 'add')
print(f'Task ID: {result.id}')
print(f'Result: {result.get(timeout=10)}')
"

# 5. è®¿é—®ç›‘æ§é¢æ¿
# http://localhost:5555
```

è¿™ä¸ªé…ç½®æŒ‡å—æ¶µç›–äº†ä»åŸºç¡€åˆ°ç”Ÿäº§çš„å®Œæ•´Celeryé…ç½®ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„é…ç½®çº§åˆ«ï¼ğŸ¯